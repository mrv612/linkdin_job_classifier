{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os dados e preparando para treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('jobs_usa_data_scientist.xlsx')\n",
    "df2 = pd.read_excel('jobs_usa_data_analyst.xlsx')\n",
    "df3 = pd.read_excel('jobs_usa_data_engineer.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['target'] = 0\n",
    "df2['target'] = 1\n",
    "df3['target'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df1.append(df2, ignore_index=True)).append(df3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/brunorosilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/brunorosilva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def tokenizar(str_texto):\n",
    "    return word_tokenize(str_texto)\n",
    "\n",
    "def limpar(lista):\n",
    "    return [i.lower() for i in lista if i.isalpha()]\n",
    "\n",
    "def sem_stops(lista):\n",
    "    return [i for i in lista if i not in stops]\n",
    "\n",
    "def stemizar(lista):\n",
    "    return [stemmer.stem(i) for i in lista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1., random_state=42) # randomizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JobDescription'] = df['JobDescription'].apply(lambda x:stemizar(sem_stops(limpar(tokenizar(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_desc = df['JobDescription'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([7891, 4144, 920, 3768, 10018, 9550, 6343, 7210, 822, 6686, 2659, 6197, 3768, 3084, 6064, 7917, 9081, 2752, 9099, 9866, 920, 3768, 10018, 3856, 8434, 9550, 6343, 7210, 822, 6686, 2659, 6197, 3768, 3084, 6064, 7917, 9081, 920, 3768, 10018, 4348, 4922, 8751, 1395, 1294, 87, 6709, 3435, 5853, 5457, 5069, 87, 6709, 3435, 5519, 4447, 9431, 1872, 5804, 7468, 3491, 9211, 7917, 1674, 3768, 87, 3640, 1794, 3112, 5051, 3768, 8714, 8450, 5516, 2905, 9477, 6931, 1180, 3698, 3854, 8410, 9484, 3680, 7372, 3134, 6709, 9093, 4729, 5605, 9136, 8845, 3467, 2659, 822, 9196, 8670, 5195, 1794, 5551, 2498, 1395, 5519, 3640, 6923, 4975, 3112, 606, 5772, 4464, 2408, 2411, 10350, 1794, 9744, 1988, 9093, 87, 8881, 1794, 1110, 6686, 7958, 2657, 5814, 9498, 2657, 8163, 3112, 6686, 7958, 2657, 3640, 7801, 4474, 5345, 5814, 87, 2882, 443, 87, 1551, 1794, 2659, 87, 5051, 10350, 1794, 5335, 7146, 9196, 1794, 5345, 2657, 5814, 5306, 3854, 5216, 87, 8410, 9431, 4557, 3854, 87, 6337, 5528, 9744, 8014, 4484, 7065, 1084, 6163, 287, 6758, 5335, 2408, 4047, 5335, 9921, 4116, 7443, 4398, 4616, 9426, 7799, 577, 8989, 1650, 8679, 4234, 7799, 8874, 2792, 7799, 3688, 8185, 577, 2446, 1226, 5431, 4750, 3688, 4349, 2316, 5195, 8874, 2792, 8322, 4557, 7081, 2316, 7799, 6844, 2850, 2792, 8185, 1301, 4349, 3768, 3856, 9237, 5051, 9196, 5335, 4349, 7799, 4349, 7801, 4474, 5041, 8997, 6769, 2657, 8185, 2446, 1226, 864, 9744, 7850, 6709, 7065, 2089, 9143, 2469, 4349, 652, 2376, 9782, 8620, 6955, 3949, 5013, 4445, 4217, 2752, 443, 2376, 6448, 1169, 2657, 3629, 3316, 2659, 1180, 5051, 1065, 3791, 170, 2752, 3316, 2376, 8322, 6709, 170, 2227, 2659, 6988, 1167, 87, 8997, 5013, 3255, 5519, 4975, 3112, 606, 5772, 4464, 2408, 2411, 10350, 1794, 9744, 1988, 9093, 87, 8881, 4872, 1110, 6686, 7958, 2657, 5814, 9498, 2657, 8163, 3112, 6686, 7958, 2657, 5345, 5814, 87, 2882, 443, 87, 1551, 1794, 2659, 87, 5051, 10350, 1794, 8801, 102, 5335, 7146, 9196, 1794, 5345, 2657, 5814, 5306, 3854, 5216, 87, 8410, 9431, 4557, 3854, 87, 6337, 5528, 9744, 8014, 4484, 7065, 1084, 6163, 287, 6758, 5335, 2408, 4047, 443, 3255, 4116, 7799, 577, 8989, 1650, 8679, 4234, 8874, 5762, 4349, 4349, 3688, 8185, 577, 8874, 9239, 6869, 4217, 3068, 5543, 4217, 2209, 4422, 4616, 5013, 8480, 3768, 4065, 4539, 920, 3768, 87, 5706, 212, 5853, 5457, 5069, 606, 212, 5069, 9537, 4447, 2665, 5805, 920, 3768, 7666, 8843, 10143, 2269, 5805, 8299, 10143, 9351, 4520, 3070, 32, 47, 3680, 126, 1876, 9481, 5565, 3499, 5434, 3712, 6519, 210, 6519, 8251, 2146, 1501, 851, 6116, 5565, 9590, 635, 2507, 3490, 4249, 7443, 2769, 7799, 5956, 10350, 5638, 4922, 10143, 920, 3768, 2313, 1591, 4447, 4966, 9622, 1389, 10349, 4616, 2554, 1107, 2099, 6114, 7262, 2313, 1065, 983, 6978, 5805, 5565, 920, 3768, 3739, 9426, 2368, 4217, 5805, 6978, 2866, 4233, 7886, 7886, 8879, 4217, 5805, 8266, 4217, 5132, 5377, 6586, 4217, 5805, 7856, 7690, 5013, 2530, 4233, 7886, 4217, 5805, 3018, 2449, 5132, 5377, 6586, 487, 965, 5519, 10226, 1730, 6, 7611, 6308, 5694, 5555, 8158, 4246, 3640, 5694, 3470, 10143, 107, 7275, 4841, 1518, 3255, 2646, 6586, 920, 3768, 4322, 9287, 5999, 1976, 6407, 2933, 9005, 2313, 2281, 9722, 6586, 8350, 9589, 2672, 7873, 7407, 7873, 10143, 2209, 5013, 2530, 6586, 2053, 3856, 6586, 2053, 8450, 2687, 3768, 8620, 8450]),\n",
       "       list([212, 3818, 2570, 438, 2913, 5013, 126, 3605, 2584, 9409, 2530, 7416, 1440, 6670, 2376, 8265, 4819, 2735, 2584, 250, 7416, 3486, 3605, 2498, 6783, 7463, 3698, 9744, 7620, 6931, 3491, 2230, 4896, 8625, 5931, 6410, 6073, 4557, 2498, 7690, 5013, 2530, 4944, 3491, 3068, 1440, 5501, 9744, 7620, 606, 2584, 126, 250, 7620, 9744, 3491, 9627, 1307, 7799, 606, 4819, 2735, 250, 6470, 7463, 2498, 7799, 864, 9920, 4908, 1014, 5911, 6955, 3698, 4398, 7799, 6783, 8418, 250, 606, 4561, 1307, 5846, 8004, 4989, 287, 8994, 993, 8843, 1346, 2672, 8235, 1210, 8410, 6284, 2498, 4557, 8411, 1226, 5846, 9744, 1818, 9700, 4989, 9844, 9744, 10350, 3491, 3640, 7620, 9744, 1065, 1000, 3491, 4819, 4577, 4539, 2530, 8625, 7168, 1294, 7861, 4819, 250, 9744, 8684, 7960, 5027, 9744, 7390, 5699, 3196, 7799, 577, 8874, 2792, 7286, 7419, 4405, 4557, 1566, 7088, 5617, 8185, 3688, 4349, 8620, 8450, 3856, 9099, 8994, 993, 7748, 3295, 6352, 6586, 8812, 1065, 9143, 5605, 9840, 2089, 10009, 3791, 5335, 2089, 9920, 864, 2250, 2089, 9920, 606, 3435, 8687, 9143, 8972, 4643, 5537, 2089, 8185, 2089, 4349, 1065, 9136, 2498, 5605, 7620, 8997, 4349, 9340, 3640, 8515, 7168, 1109, 7387, 3566, 2089, 7592, 4349, 8185, 9744, 7620, 2089, 1766, 4372, 2485, 1424, 7912, 7021, 9786, 9328, 7748, 9744, 6709, 904, 1161, 7014, 655, 3078, 5988, 5805, 4695, 9744, 7748, 2053, 3791, 4473, 8271, 7463, 9920, 652, 8265, 9744, 822, 4246, 1084, 212, 5506, 6875, 606, 5377, 1335, 2752, 2105, 2291, 8987, 5401, 5903, 2059, 4285, 8881, 7873, 4349, 5377, 3196, 5846, 3640, 2105, 1104, 5377, 9321, 1104, 5377, 3280, 3018, 9431, 443, 2313, 431, 7348, 1335, 2752, 2105, 1383, 7666, 8843, 1199, 4246, 10143, 3490, 9136, 4249, 8154, 7443, 8542, 2269, 5805, 3822, 342, 10143, 9351, 4520, 3070, 32, 1876, 3499, 5434, 3712, 6519, 210, 3680, 126, 1501, 6116, 5565, 851, 8480, 2269, 3018, 1501, 1501, 6116, 9631, 6923, 9920, 982, 195, 6114, 4021, 10189, 5638, 4927, 8350, 9589, 2672, 7873, 6318, 10143, 2209, 5013, 2530, 6586, 2053, 3856, 6586, 2053, 8450, 5560, 8620, 8450]),\n",
       "       list([1335, 8680, 4285, 5563, 8666, 9956, 996, 5013, 5553, 10054, 2858, 8658, 6639, 982, 431, 982, 9305, 1591, 2554, 5999, 5401, 9689, 3688, 2348, 2554, 9909, 9431, 5914, 3688, 1718, 6202, 4485, 1569, 4838, 6325, 9431, 398, 9577, 4883, 4217, 5094, 3688, 904, 8625, 5310, 3295, 5941, 10022, 6917, 8350, 3461, 10193, 758, 3430, 8004, 10034, 3295, 1998, 5992, 9840, 7281, 9804, 6313, 10325, 1294, 2554, 9431, 8839, 8542, 33, 5401, 1335, 8680, 4285, 3698, 5563, 8666, 9956, 5506, 8002, 5419, 5013, 1902, 4429, 367, 3552, 9744, 7419, 8625, 72, 758, 8997, 4908, 3854, 7168, 305, 6586, 8689, 4013, 6337, 9744, 3112, 7573, 1784, 9744, 3899, 1065, 6470, 2089, 1794, 4047, 5335, 4984, 5814, 9744, 1026, 3491, 250, 8418, 9744, 4195, 5740, 443, 5335, 9744, 1167, 9044, 2225, 9340, 5335, 250, 3112, 9744, 4195, 443, 5376, 31, 8158, 6639, 982, 431, 982, 9305, 8350, 9589, 2672, 7873, 6318, 10143, 2209, 5013, 2530, 6586, 2053, 3856, 6586, 2053, 8450, 7286, 904]),\n",
       "       ...,\n",
       "       list([9975, 181, 5013, 3738, 6627, 5769, 2693, 3869, 102, 6627, 3196, 2177, 9409, 7856, 9658, 6713, 3435, 6073, 4557, 5605, 4447, 7468, 9744, 6549, 5941, 4246, 918, 6733, 8715, 3018, 5519, 1339, 1582, 2053, 3435, 4944, 3491, 3435, 9658, 6713, 3435, 9829, 5551, 5814, 2084, 5306, 4572, 2071, 9954, 9744, 9196, 4819, 4944, 2752, 5846, 7799, 864, 10017, 606, 2530, 4557, 1818, 7561, 9782, 8997, 3640, 7545, 1582, 758, 10350, 3491, 5501, 3688, 4557, 2882, 606, 4349, 1000, 2792, 9744, 5335, 3491, 2792, 7286, 7419, 6586, 250, 250, 5805, 4557, 3491, 9744, 7419, 3552, 1401, 7088, 5762, 4557, 4349, 1000, 2792, 4557, 3491, 9744, 7419, 7286, 7419, 9328, 8185, 7505, 4349, 4349, 3999, 9744, 2530, 2752, 4561, 8625, 8325, 9744, 7125, 3791, 2752, 8410, 5756, 4944, 443, 2752, 7799, 3078, 4944, 6586, 250, 3629, 4681, 3854, 1794, 758, 4819, 5418, 4582, 7487, 7004, 1356, 6586, 7561, 7799, 775, 9658, 6713, 6586, 7561, 5814, 250, 8265, 4666, 9744, 1395, 2691, 606, 1975, 1339, 6205, 4572, 2530, 9099, 5941, 7545, 8029, 606, 9484, 3435, 982, 4557, 8712, 6677, 9744, 1339, 9711, 3435, 2586, 8029, 4195, 6057, 8298, 9829, 9921, 9744, 1675, 9136, 287, 3552, 5756, 4539, 1339, 9711, 9484, 363, 2530, 3435, 8235, 4944, 9744, 2498, 72, 9744, 5195, 1708, 2530, 1818, 9484, 8997, 5335, 250, 6460, 9658, 6713, 3134, 7416, 6627, 5632, 8996, 9239, 10350, 6627, 5632, 8996, 2059, 8997, 401, 5519, 8997, 3791, 9552, 7573, 2225, 8029, 7545, 1065, 5095, 4572, 918, 7004, 1356, 4557, 5077, 4561, 6627, 8843, 4896, 1794, 7450, 6, 1933, 3856, 33, 5377, 7709, 3640, 3705, 1335, 6123, 5373, 9328, 3490, 9627, 8010, 1654, 8687, 6318, 233, 349, 10350, 5250, 606, 8843, 606, 363, 2530, 9576, 349, 8410, 7994, 9209, 822, 10136, 8350, 9589, 2672, 7873, 7407, 7873, 10143, 2209, 5013, 2530, 7620, 6586, 2053, 3856, 3450, 831, 3450, 8997, 8994, 424]),\n",
       "       list([2672, 4557, 1395, 606, 7416, 959, 9711, 4557, 758, 2672, 4557, 1395, 10350, 9687, 1696, 8450, 5772, 5560, 9081, 5519, 6464, 6283, 5560, 9081, 2504, 6163, 6708, 4557, 4908, 2504, 4557, 9782, 7799, 3751, 4557, 4908, 443, 10012, 9914, 1903, 3751, 6955, 4616, 9426, 287, 4557, 8004, 1065, 4447, 3491, 918, 8453, 9088, 3818, 1084, 2162, 10124, 2672, 1872, 389, 3195, 9658, 6713, 8997, 1689, 8265, 10350, 3552, 6955, 6114, 6720, 4356, 577, 8874, 2792, 4557, 7286, 7419, 7088, 2316, 4405, 9754, 3688, 4349, 6642, 1766, 6496, 5335, 4687, 864, 6642, 9476, 2089, 3544, 9143, 864, 3491, 2089, 864, 759, 1902, 4557, 4016, 4047, 8968, 3435, 3712, 9920, 6501, 4908, 8843, 7450, 3134, 771, 758, 9920, 10091, 5335, 2376, 5134, 9143, 5757, 4643, 7065, 2089, 4681, 2250, 2089, 9920, 9136, 5772, 1346, 7873, 4539, 606, 3078, 6148, 9078, 8185, 577, 3078, 5975, 3856, 3544, 3213, 7608, 3078, 9687, 250, 4349, 606, 6811, 6872, 3435, 8620, 3856, 4349, 8350, 9589, 2672, 7873, 5805, 10143, 2209, 5013, 2530, 6586, 2053, 3856, 8620, 8450, 6586, 2053, 8450]),\n",
       "       list([4851, 9744, 10053, 8968, 4557, 6709, 9744, 6709, 2657, 9136, 10350, 7561, 4851, 3491, 4538, 3435, 3018, 673, 7125, 6734, 9431, 3379, 5250, 2846, 5891, 8687, 6338, 606, 4557, 3435, 7856, 4557, 3147, 5654, 4557, 8004, 9621, 6163, 2530, 4557, 7799, 8625, 8265, 2657, 9051, 5201, 2469, 7065, 606, 9782, 3435, 6448, 2316, 9340, 7861, 8625, 9782, 9744, 7799, 4557, 7799, 9136, 9744, 2657, 7014, 655, 1065, 425, 967, 606, 5805, 10350, 8997, 5335, 7620, 3435, 9374, 4649, 4874, 4021, 425, 8997, 9334, 7125, 6734, 9431, 3379, 5250, 2846, 2168, 8687, 443, 9744, 1064, 2810, 4874, 1372, 6250, 2657, 8160, 5941, 1938, 4447, 10350, 9123, 8004, 7450, 8874, 2792, 7286, 7419, 7620, 4557, 5762, 4349, 3688, 340, 9920, 2316, 9744, 6709, 5941, 3491, 9744, 2657, 4349, 606, 9744, 9340, 4349, 1766, 7014, 655, 9143, 6209, 2250, 4643, 8972, 5537, 2089, 4349, 4538, 425, 8185, 4349, 606, 2376, 2168, 8687, 759, 1902, 5078, 8160, 8684, 9920, 4091, 606, 5419, 9921, 3791, 2377, 9920, 606, 3949, 7856, 3435, 7416, 4539, 8350, 9589, 2672, 7873, 7873, 10143, 2209, 5013, 2530, 7620, 6586, 2053, 3856, 7286, 904])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set([p for sent in jobs_desc for p in sent])\n",
    "\n",
    "indices_de_palavras = {palavra: e+1 for e, palavra in enumerate(vocab)}  # e+1 para que o primeiro índice não seja 0, que é um pad\n",
    "\n",
    "vetores_msg = np.array([[indices_de_palavras[p] for p in d] for d in jobs_desc], dtype=object)\n",
    "vetores_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizar(matriz_int, dim=len(vocab)+1):\n",
    "    binarizado = np.zeros((len(matriz_int), dim))\n",
    "\n",
    "    for e, vetor in enumerate(matriz_int):\n",
    "        binarizado[e, vetor] = 1.\n",
    "\n",
    "    return binarizado\n",
    "\n",
    "vetores_msg_bin = binarizar(vetores_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_bin = binarizar(etiquetas, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(vetores_msg_bin, etiquetas_bin, train_size=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning e criação do Relatório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objects as go\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.io import to_html\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, activation, optimizer, vocab):\n",
    "    modelo = models.Sequential()\n",
    "    for i in range(len(units)):\n",
    "        modelo.add(layers.Dense(units[i], activation=activation, input_shape=(len(vocab)+1,)))\n",
    "    \n",
    "    modelo.add(layers.Dense(3, activation='softmax'))\n",
    "    modelo.compile(optimizer=optimizer, \n",
    "            loss=\"categorical_crossentropy\", \n",
    "            metrics='accuracy')\n",
    "    return modelo\n",
    "def train_model(modelo, train_x, train_y, test_x, test_y, batch):\n",
    "    aprendeu_parou = callbacks.EarlyStopping(\n",
    "        min_delta=0.001,  \n",
    "        patience=5,  \n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    historia = modelo.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        epochs=200, \n",
    "        batch_size=batch, \n",
    "        validation_data=(test_x, test_y),\n",
    "        callbacks=[aprendeu_parou],\n",
    "        verbose=0\n",
    "    )\n",
    "    return historia\n",
    "    \n",
    "def plot_results(historia):\n",
    "\n",
    "    dic_historia = historia.history  # dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "    perda_treino = dic_historia['loss']\n",
    "    perda_valid = dic_historia['val_loss']\n",
    "\n",
    "    acuracia_treino = dic_historia['accuracy'] \n",
    "    acuracia_valid = dic_historia['val_accuracy']\n",
    "    epocas = range(1, len(acuracia_treino) + 1)\n",
    "    tmp = pd.DataFrame({\n",
    "        \"Epocas\":epocas,\n",
    "        \"Acurácia_Treino\":acuracia_treino,\n",
    "        \"Acurácia_Validação\":acuracia_valid,\n",
    "        \"Custo_Treino\":perda_treino,\n",
    "        \"Custo_Validação\":perda_valid,\n",
    "    })\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1,\n",
    "        subplot_titles=(\"Acurácia por Épocas\", \"Custo por Épocas\"))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tmp['Epocas'],\n",
    "            y=tmp[\"Acurácia_Validação\"],\n",
    "            name=\"Acurácia Validação\",\n",
    "            mode='lines+markers'\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tmp['Epocas'],\n",
    "            y=tmp[\"Acurácia_Treino\"],\n",
    "            name=\"Acurácia Treino\",\n",
    "            mode='lines+markers'\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tmp['Epocas'],\n",
    "            y=tmp[\"Custo_Validação\"],\n",
    "            name=\"Custo Validação\",\n",
    "            mode='lines+markers'\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tmp['Epocas'],\n",
    "            y=tmp[\"Custo_Treino\"],\n",
    "            name=\"Custo Treino\",\n",
    "            mode='lines+markers'\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Épocas\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Épocas\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Acurácia\", row=1, col=1, range=[0, 1])\n",
    "    fig.update_yaxes(title_text=\"Custo\", row=2, col=1, range=[0, 1])\n",
    "    fig.update_layout(height=1200, width=800)\n",
    "    fig_html = to_html(fig, full_html=False)\n",
    "    return fig_html\n",
    "\n",
    "def build_report(historia, hiperparams, modelo, teste_x, teste_y, first_run=False, last_run=False):\n",
    "    if first_run:\n",
    "        fl = open(\"tuning-process-report.html\", 'w')\n",
    "        fl.write(\"\"\"\n",
    "        <html>\n",
    "            <head>\n",
    "                <meta charset=\"UTF-8\">\n",
    "                <style>\n",
    "                .plotly-graph-div {\n",
    "                    margin: 0 auto;\n",
    "                }\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1 align=\\'center\\'>Report do processo de busca do melhor modelo</h1>\n",
    "                <hr>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        fl = open('tuning-process-report.html', 'a')\n",
    "        fl.write(\"<hr>\")\n",
    "\n",
    "    fl.write(\"\"\"\n",
    "    <h2 align=\\'center\\'>Hiper parâmetros do modelo</h2>\n",
    "    <div align=\\'center\\'>\n",
    "    \"\"\")\n",
    "    for hiperparam in hiperparams:\n",
    "        fl.write(str(hiperparam) + \": \" + str(hiperparams[hiperparam]) + \"<br>\")\n",
    "    \n",
    "    avaliacao = modelo.evaluate(teste_x, teste_y)\n",
    "    fl.write(f'''\n",
    "    Acurácia na avaliação: {str(avaliacao[1])} <br>\n",
    "    Perda: {str(avaliacao[0])}\n",
    "    </div>\n",
    "    ''')\n",
    "    fig_html = plot_results(historia)\n",
    "    fl.write(fig_html)\n",
    "    if last_run:\n",
    "        fl.write(\"\"\"\n",
    "        </body>\n",
    "        </html>        \n",
    "        \"\"\")\n",
    "    fl.close()\n",
    "    return avaliacao[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparams_grid = {\n",
    "    \"units\":[[1], [8], [16, 16], [8, 8, 8, 8, 8, 8, 8, 8], [32, 64, 8], [128, 64], [128, 64, 8]],\n",
    "    \"activations\":[\"tanh\", \"relu\"],\n",
    "    \"batches\":[16, 64, 512],\n",
    "    \"optimizers\":[\"SGD\", \"rmsprop\", \"adam\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 4ms/step - loss: 0.3876 - accuracy: 0.8531\n",
      "New best ACC 85.31 using ([16], 'tanh', 16, 'SGD')\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.8306\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8495\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8318\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8649\n",
      "New best ACC 86.49 using ([8, 8, 8], 'tanh', 16, 'SGD')\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.8389\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8460\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8424\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.4091 - accuracy: 0.8400\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.4654 - accuracy: 0.8152\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.3963 - accuracy: 0.8649\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "first_run = True\n",
    "last_run = False\n",
    "max_runs = 1\n",
    "best_acc = 0\n",
    "\n",
    "for f in hiperparams_grid:\n",
    "    max_runs *= len(hiperparams_grid[f])\n",
    "\n",
    "for h_units in hiperparams_grid[\"units\"]:\n",
    "    for h_activation in hiperparams_grid[\"activations\"]:\n",
    "        for h_batch in hiperparams_grid['batches']:\n",
    "            for h_optimizer in hiperparams_grid['optimizers']:\n",
    "                if max_runs == 1:\n",
    "                    last_run=True\n",
    "                modelo = build_model(h_units, h_activation, h_optimizer, vocab)\n",
    "                historia = train_model(modelo, train_x, train_y, test_x, test_y, h_batch)\n",
    "                current_acc = build_report(historia,\n",
    "                             {\"units\":h_units, \"activation\":h_activation, \"batch\":h_batch, \"optimizer\":h_optimizer}, \n",
    "                             modelo, test_x, test_y, first_run, last_run)\n",
    "                if current_acc > best_acc:\n",
    "                    best_acc = current_acc\n",
    "                    print(f\"New best ACC {round(best_acc*100, 2)} using {h_units, h_activation, h_batch, h_optimizer}\")\n",
    "                first_run=False\n",
    "                max_runs-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o melhor modelo\n",
    "Foram feitos diversos testes e o melhor modelo encontrado foi o definido a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = models.Sequential([\n",
    "    layers.Dense(8, activation='tanh', input_shape=(len(vocab)+1,)),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(8, activation='tanh'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna.compile(optimizer='SGD', loss='categorical_crossentropy', metrics='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = rna.fit(train_x, train_y, epochs=21, validation_data=(test_x, test_y), batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "rna.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o vocab e os índices\n",
    "fl = open(\"vocab.txt\", 'w')\n",
    "fl.write(str(vocab))\n",
    "fl.close()\n",
    "fl = open(\"indice_de_palavras.txt\", \"w\")\n",
    "fl.write(str(indices_de_palavras))\n",
    "fl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
